# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v7AtWo9sQHGC8TDL3YmkiyEWnxef8klx
"""

import pandas as pd
df=pd.read_csv('sample_data/mnist_train_small.csv',header=None)

df

X=df.iloc[:,1:].values
y=df.iloc[:,0].values

import numpy as np
np.unique(y)

df[0].value_counts()

import matplotlib.pyplot as plt
plt.imshow(X[0].reshape(28,28))
plt.show()

import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

def output_neuron(X,w,b):
  return tf.matmul(X,w)+b

def activation(ynet):
  return tf.nn.softmax(ynet)

def loss(y_true,logits):
  return tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(y_true,logits))

X=df.iloc[:,1:].values
y=df.iloc[:,0].values
sc=StandardScaler()
X_new=sc.fit_transform(X)

tf.random.set_seed(10)

w=tf.Variable(tf.random.truncated_normal(shape=[784,10],dtype='double'))
b=tf.Variable(tf.random.truncated_normal(shape=[10],dtype='double'))

optimizer=tf.optimizers.SGD(learning_rate=.1)
for epoch in range(2000):
  with tf.GradientTape() as tape:
    ynet=output_neuron(X_new,w,b)
    logits=activation(ynet)
    ls=loss(y,logits)
    yhat=tf.argmax(logits,1)
    if(epoch%50==0):
      print("Loss:",ls,"\t","Score:",accuracy_score(y,yhat))
    gradients=tape.gradient(ls,[w,b])
    optimizer.apply_gradients(zip(gradients,[w,b]))

