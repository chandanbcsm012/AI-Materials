# -*- coding: utf-8 -*-
"""26June_Regression_HiddenLayer1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yrwsSqDZPdY-qUprEFMeATfDt3GQxEZ7
"""

import pandas as pd
import tensorflow as tf
from sklearn.metrics import r2_score

df=pd.read_csv('sample_data/california_housing_train.csv')
X=df.iloc[:,:-1].values
y=df.iloc[:,-1].values

def output_neuron(X,w,b):
  return tf.matmul(X,w)+b

def loss(y_true,y_pred):
  return tf.losses.mean_squared_error(y_true,y_pred)

def activation(ynet):
  return tf.nn.sigmoid(ynet)

tf.random.set_seed(10)

wh=tf.Variable(tf.random.truncated_normal(shape=[8,25],dtype=tf.double))
bh=tf.Variable(tf.random.truncated_normal(shape=[1,25],dtype=tf.double))

wo=tf.Variable(tf.random.truncated_normal(shape=[25,1],dtype=tf.double))
bo=tf.Variable(tf.random.truncated_normal(shape=[1,1],dtype=tf.double))

df=pd.read_csv('sample_data/california_housing_train.csv')
X=df.iloc[:,:-1].values
y=df.iloc[:,-1].values
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_new=sc.fit_transform(X)
optimizer=tf.optimizers.SGD(learning_rate=.1)
for epoch in range(2000):
  with tf.GradientTape() as tape:
    ynet=output_neuron(X_new,wh,bh)
    logit=activation(ynet)

    yhat=output_neuron(logit,wo,bo)
    yhat=tf.reshape(yhat,[-1])#flatten the tensor yhat
    ls=loss(y,yhat)
    if(epoch%100==0):
      print("Loss:",ls,"\t","Score:",r2_score(y,yhat.numpy()))
    dwh,dbh,dwo,dbo=tape.gradient(ls,[wh,bh,wo,bo])
    optimizer.apply_gradients(zip([dwh,dbh,dwo,dbo],[wh,bh,wo,bo]))

tf.random.set_seed(10)

wh=tf.Variable(tf.random.truncated_normal(shape=[8,25],dtype=tf.double))
bh=tf.Variable(tf.random.truncated_normal(shape=[25],dtype=tf.double))

wo=tf.Variable(tf.random.truncated_normal(shape=[25,1],dtype=tf.double))
bo=tf.Variable(tf.random.truncated_normal(shape=[1],dtype=tf.double))

df=pd.read_csv('sample_data/california_housing_train.csv')
X=df.iloc[:,:-1].values
y=df.iloc[:,-1].values
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_new=sc.fit_transform(X)
optimizer=tf.optimizers.SGD(learning_rate=.1)
for epoch in range(2000):
  with tf.GradientTape() as tape:
    ynet=output_neuron(X_new,wh,bh)
    logit=activation(ynet)

    yhat=output_neuron(logit,wo,bo)
    yhat=tf.reshape(yhat,[-1])#flatten the tensor yhat
    ls=loss(y,yhat)
    if(epoch%100==0):
      print("Loss:",ls,"\t","Score:",r2_score(y,yhat.numpy()))
    dwh,dbh,dwo,dbo=tape.gradient(ls,[wh,bh,wo,bo])
    optimizer.apply_gradients(zip([dwh,dbh,dwo,dbo],[wh,bh,wo,bo]))

tf.random.set_seed(10)

wh=tf.Variable(tf.random.truncated_normal(shape=[8,25],dtype=tf.double))
bh=tf.Variable(tf.random.truncated_normal(shape=[25],dtype=tf.double))

wo=tf.Variable(tf.random.truncated_normal(shape=[25,1],dtype=tf.double))
bo=tf.Variable(tf.random.truncated_normal(shape=[1],dtype=tf.double))

df=pd.read_csv('sample_data/california_housing_train.csv')
X=df.iloc[:,:-1].values
y=df.iloc[:,-1].values
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_new=sc.fit_transform(X)
optimizer=tf.optimizers.SGD(learning_rate=.1)
for epoch in range(2000):
  with tf.GradientTape() as tape:
    ynet=output_neuron(X_new,wh,bh)
    logit=activation(ynet)

    yhat=output_neuron(logit,wo,bo)
    yhat=tf.reshape(yhat,[-1])#flatten the tensor yhat
    ls=loss(y,yhat)
    if(epoch%100==0):
      print("Loss:",ls,"\t","Score:",r2_score(y,yhat.numpy()))
    gradients=tape.gradient(ls,[wh,bh,wo,bo])
    optimizer.apply_gradients(zip(gradients,[wh,bh,wo,bo]))

tf.random.set_seed(10)

df=pd.read_csv('sample_data/california_housing_train.csv')
X=df.iloc[:,:-1].values
y=df.iloc[:,-1].values
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
pf=PolynomialFeatures(degree=2)
sc=StandardScaler()
X_new=sc.fit_transform(pf.fit_transform(X))

wh=tf.Variable(tf.random.truncated_normal(shape=[X_new.shape[1],70],dtype=tf.double))
bh=tf.Variable(tf.random.truncated_normal(shape=[70],dtype=tf.double))

wo=tf.Variable(tf.random.truncated_normal(shape=[70,1],dtype=tf.double))
bo=tf.Variable(tf.random.truncated_normal(shape=[1],dtype=tf.double))

optimizer=tf.optimizers.SGD(learning_rate=.01)
for epoch in range(2000):
  with tf.GradientTape() as tape:
    ynet=output_neuron(X_new,wh,bh)
    logit=activation(ynet)

    yhat=output_neuron(logit,wo,bo)
    yhat=tf.reshape(yhat,[-1])#flatten the tensor yhat
    ls=loss(y,yhat)
    if(epoch%100==0):
      print("Loss:",ls,"\t","Score:",r2_score(y,yhat.numpy()))
    gradients=tape.gradient(ls,[wh,bh,wo,bo])
    optimizer.apply_gradients(zip(gradients,[wh,bh,wo,bo]))

def activation(ynet):
  return tf.nn.tanh(ynet)

tf.random.set_seed(10)

df=pd.read_csv('sample_data/california_housing_train.csv')
X=df.iloc[:,:-1].values
y=df.iloc[:,-1].values
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
pf=PolynomialFeatures(degree=3)
sc=StandardScaler()
X_new=sc.fit_transform(pf.fit_transform(X))

wh1=tf.Variable(tf.random.truncated_normal(shape=[X_new.shape[1],70],dtype=tf.double))
bh1=tf.Variable(tf.random.truncated_normal(shape=[70],dtype=tf.double))

wh2=tf.Variable(tf.random.truncated_normal(shape=[70,30],dtype=tf.double))
bh2=tf.Variable(tf.random.truncated_normal(shape=[30],dtype=tf.double))

wo=tf.Variable(tf.random.truncated_normal(shape=[30,1],dtype=tf.double))
bo=tf.Variable(tf.random.truncated_normal(shape=[1],dtype=tf.double))

optimizer=tf.optimizers.SGD(learning_rate=.01)
for epoch in range(1000):
  with tf.GradientTape() as tape:
    ynet1=output_neuron(X_new,wh1,bh1)
    logit1=activation(ynet1)

    ynet2=output_neuron(logit1,wh2,bh2)
    logit2=activation(ynet2)

    yhat=output_neuron(logit2,wo,bo)
    yhat=tf.reshape(yhat,[-1])#flatten the tensor yhat
    ls=loss(y,yhat)
    if(epoch%100==0):
      print("Loss:",ls,"\t","Score:",r2_score(y,yhat.numpy()))
    gradients=tape.gradient(ls,[wh1,bh1,wh2,bh2,wo,bo])
    optimizer.apply_gradients(zip(gradients,[wh1,bh1,wh2,bh2,wo,bo]))

