# -*- coding: utf-8 -*-
"""28june train test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12-8x_4pzLm4MZXtquofGLmY4cTB6y6_9

**sparse_categorical_crossentropy(y_true,logits):** 

here, y_true ----> 1d tensor of actual output

logits---> 2d array of pribabilities

returns  1d tensor of loss where each element represents loss of individual sample i.e we may use mean of this tensor to get total loss

**categorical_crossentropy(y_true,logits):** 

here, y_true ----> 2d tensor of actual output as one hot encoding

logits---> 2d array of pribabilities

returns  1d tensor of loss where each element represents loss of individual sample i.e we may use mean of this tensor to get total loss.

**how to convert target in one hot encoding:**

tf.one_hot(target,classes)

**prediction on test data:**



*   only one forward pass is used for test data
*   use last  update weight and bias
"""

# perceptron
import pandas as pd
import numpy as np
df=pd.read_csv('sample_data/mnist_train_small.csv',header=None)
df.head()
X=df.iloc[:,1:].values
y=df.iloc[:,0].values
df[0].value_counts()
import tensorflow as tf
def output_neuron(X,w,b):
  return tf.matmul(X,w)+b

def activation(ynet):
  return tf.nn.softmax(ynet)

def loss(y_true,logits): # logits 2d array of prob
  return tf.reduce_mean(tf.losses.categorical_crossentropy(y_true,logits))

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_new=sc.fit_transform(X)
tf.random.set_seed(3)
w=tf.Variable(tf.random.truncated_normal(shape=[784,10],dtype='double'))
b=tf.Variable(tf.random.truncated_normal(shape=[10],dtype='double'))
optimizer=tf.optimizers.SGD(learning_rate=0.1)
for epoch in range(5000):
 with tf.GradientTape() as tape:
    ynet=output_neuron(X_new,w,b)
    logits=activation(ynet)
    ls=loss(tf.one_hot(y,10),logits)
    from sklearn.metrics import accuracy_score
    yhat=tf.argmax(logits,1)
    if epoch%500==0:
      print('ls:',ls,'\t','score',accuracy_score(y,yhat))
    grad=tape.gradient(ls,[w,b])
    optimizer.apply_gradients(zip(grad,[w,b]))

#testing data
df_test=pd.read_csv('sample_data/mnist_test.csv',header=None)
X_test=df.iloc[:,1:].values
y_test=df.iloc[:,0].values
X_test_new=sc.transform(X_test)
ynet=output_neuron(X_test_new,w,b)
logits=activation(ynet)
ls=loss(tf.one_hot(y_test,10),logits)
yhat=tf.argmax(logits,1)
print('score:',accuracy_score(y_test,yhat))

import cv2
gray=cv2.imread('0.jpeg',0)
gray=gray.reshape(1,-1)
gray=sc.transform(gray)
ynet=output_neuron(gray,w,b)
logits=activation(ynet)
pred=tf.argmax(logits,1)
print(pred)

