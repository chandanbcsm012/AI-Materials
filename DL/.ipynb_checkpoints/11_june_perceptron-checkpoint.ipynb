{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('../../dataset/regression/salary_1_variable.csv')\n",
    "X=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 12804.48, NNZs: 1, Bias: 3202.431178, T: 30, Avg. loss: 286000922.549630\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12097.60, NNZs: 1, Bias: 3671.715680, T: 60, Avg. loss: 74136639.480246\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12414.52, NNZs: 1, Bias: 4206.599643, T: 90, Avg. loss: 73248779.362612\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 12614.01, NNZs: 1, Bias: 4680.461974, T: 120, Avg. loss: 68658680.703398\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12317.82, NNZs: 1, Bias: 5043.391181, T: 150, Avg. loss: 67609951.250977\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 12859.24, NNZs: 1, Bias: 5486.597038, T: 180, Avg. loss: 62289666.872158\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 12373.99, NNZs: 1, Bias: 5771.877381, T: 210, Avg. loss: 64526274.005004\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 12780.72, NNZs: 1, Bias: 6165.696376, T: 240, Avg. loss: 59947447.414042\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 12631.22, NNZs: 1, Bias: 6468.854122, T: 270, Avg. loss: 60722963.240244\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 12310.91, NNZs: 1, Bias: 6740.548596, T: 300, Avg. loss: 59890688.068376\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 12394.02, NNZs: 1, Bias: 7045.976026, T: 330, Avg. loss: 57400782.338059\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 12040.20, NNZs: 1, Bias: 7280.323129, T: 360, Avg. loss: 56298668.407106\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 11943.46, NNZs: 1, Bias: 7546.176985, T: 390, Avg. loss: 55390670.493570\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 11847.31, NNZs: 1, Bias: 7800.385097, T: 420, Avg. loss: 53483634.247393\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 11684.23, NNZs: 1, Bias: 8031.536907, T: 450, Avg. loss: 51628635.172973\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 11781.80, NNZs: 1, Bias: 8297.867683, T: 480, Avg. loss: 51722685.610144\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 11421.35, NNZs: 1, Bias: 8465.087828, T: 510, Avg. loss: 46089471.007505\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 11793.67, NNZs: 1, Bias: 8760.108122, T: 540, Avg. loss: 51139650.826435\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 11675.92, NNZs: 1, Bias: 8971.875029, T: 570, Avg. loss: 48762388.665877\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 11789.51, NNZs: 1, Bias: 9217.597341, T: 600, Avg. loss: 48124916.612075\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 12011.76, NNZs: 1, Bias: 9466.562065, T: 630, Avg. loss: 46084014.430772\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 11845.58, NNZs: 1, Bias: 9650.818242, T: 660, Avg. loss: 46210127.188465\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 12070.54, NNZs: 1, Bias: 9881.889840, T: 690, Avg. loss: 43817322.377902\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 11845.46, NNZs: 1, Bias: 10051.649290, T: 720, Avg. loss: 45350725.922482\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 12032.24, NNZs: 1, Bias: 10276.957759, T: 750, Avg. loss: 43167620.897390\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 11875.09, NNZs: 1, Bias: 10445.918996, T: 780, Avg. loss: 43961917.743480\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 11710.75, NNZs: 1, Bias: 10612.327175, T: 810, Avg. loss: 43252151.334556\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 11607.59, NNZs: 1, Bias: 10784.926499, T: 840, Avg. loss: 42120135.439169\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 11532.58, NNZs: 1, Bias: 10953.753243, T: 870, Avg. loss: 41470445.383071\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 11491.52, NNZs: 1, Bias: 11125.871890, T: 900, Avg. loss: 41205643.530525\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 11456.29, NNZs: 1, Bias: 11298.617942, T: 930, Avg. loss: 40509005.797018\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 11342.80, NNZs: 1, Bias: 11455.453320, T: 960, Avg. loss: 39128591.937197\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 11331.35, NNZs: 1, Bias: 11619.894306, T: 990, Avg. loss: 38998856.555489\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 11149.78, NNZs: 1, Bias: 11750.981697, T: 1020, Avg. loss: 37188949.750542\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 11416.77, NNZs: 1, Bias: 11952.212779, T: 1050, Avg. loss: 38804447.185437\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 11270.12, NNZs: 1, Bias: 12087.259274, T: 1080, Avg. loss: 37572455.615432\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 11344.50, NNZs: 1, Bias: 12256.020351, T: 1110, Avg. loss: 37573215.104359\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 11427.46, NNZs: 1, Bias: 12424.740799, T: 1140, Avg. loss: 36889730.924167\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 11360.88, NNZs: 1, Bias: 12565.305532, T: 1170, Avg. loss: 36360607.396069\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 11591.61, NNZs: 1, Bias: 12744.750461, T: 1200, Avg. loss: 34496180.227355\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 11516.21, NNZs: 1, Bias: 12874.592473, T: 1230, Avg. loss: 35581958.041433\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 11588.67, NNZs: 1, Bias: 13028.204182, T: 1260, Avg. loss: 34422049.702960\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 11441.37, NNZs: 1, Bias: 13146.088969, T: 1290, Avg. loss: 35039759.953727\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 11377.76, NNZs: 1, Bias: 13275.858394, T: 1320, Avg. loss: 34436554.158767\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 11292.34, NNZs: 1, Bias: 13396.105574, T: 1350, Avg. loss: 33828037.028557\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 11158.32, NNZs: 1, Bias: 13510.299365, T: 1380, Avg. loss: 33455151.116436\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 11102.28, NNZs: 1, Bias: 13633.460302, T: 1410, Avg. loss: 33156622.823594\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 10996.34, NNZs: 1, Bias: 13747.967451, T: 1440, Avg. loss: 31862614.317280\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 11018.23, NNZs: 1, Bias: 13879.379328, T: 1470, Avg. loss: 32233885.156683\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 11071.04, NNZs: 1, Bias: 14010.740104, T: 1500, Avg. loss: 32281993.265201\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 10911.65, NNZs: 1, Bias: 14112.049961, T: 1530, Avg. loss: 30929501.277402\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 11124.66, NNZs: 1, Bias: 14264.158562, T: 1560, Avg. loss: 31751208.235887\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 10989.64, NNZs: 1, Bias: 14362.764390, T: 1590, Avg. loss: 31006572.938615\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 11077.21, NNZs: 1, Bias: 14495.115549, T: 1620, Avg. loss: 30905289.594443\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 11152.32, NNZs: 1, Bias: 14624.677804, T: 1650, Avg. loss: 30428241.188201\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 11064.87, NNZs: 1, Bias: 14723.140435, T: 1680, Avg. loss: 30326890.363262\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 11160.46, NNZs: 1, Bias: 14851.090129, T: 1710, Avg. loss: 29672841.991311\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 11061.79, NNZs: 1, Bias: 14947.459808, T: 1740, Avg. loss: 29876202.678894\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 11197.48, NNZs: 1, Bias: 15077.095520, T: 1770, Avg. loss: 28953971.149078\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 11149.50, NNZs: 1, Bias: 15177.038427, T: 1800, Avg. loss: 29305326.958915\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 11150.37, NNZs: 1, Bias: 15282.879692, T: 1830, Avg. loss: 28842933.657289\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 11026.83, NNZs: 1, Bias: 15371.177200, T: 1860, Avg. loss: 28993273.861167\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 10870.15, NNZs: 1, Bias: 15452.119556, T: 1890, Avg. loss: 28284994.912573\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 10887.42, NNZs: 1, Bias: 15557.892378, T: 1920, Avg. loss: 28373371.782758\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 10808.41, NNZs: 1, Bias: 15646.274060, T: 1950, Avg. loss: 27645947.680481\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 10753.89, NNZs: 1, Bias: 15739.261670, T: 1980, Avg. loss: 27570252.391633\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 10748.09, NNZs: 1, Bias: 15836.272374, T: 2010, Avg. loss: 27461025.306405\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 10640.63, NNZs: 1, Bias: 15915.383502, T: 2040, Avg. loss: 26487976.495660\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 10827.06, NNZs: 1, Bias: 16038.246015, T: 2070, Avg. loss: 27462328.424941\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 10766.50, NNZs: 1, Bias: 16125.769862, T: 2100, Avg. loss: 26791100.532923\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 10917.38, NNZs: 1, Bias: 16240.878923, T: 2130, Avg. loss: 26018851.057080\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 10921.14, NNZs: 1, Bias: 16335.283045, T: 2160, Avg. loss: 26395779.547928\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 10844.21, NNZs: 1, Bias: 16410.098461, T: 2190, Avg. loss: 26227972.566991\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 10963.30, NNZs: 1, Bias: 16516.661304, T: 2220, Avg. loss: 25442465.397458\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 10852.07, NNZs: 1, Bias: 16587.890009, T: 2250, Avg. loss: 26150018.940352\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 10925.27, NNZs: 1, Bias: 16685.327813, T: 2280, Avg. loss: 25495900.095472\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 10820.33, NNZs: 1, Bias: 16757.088177, T: 2310, Avg. loss: 25705610.264886\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 10776.09, NNZs: 1, Bias: 16836.774725, T: 2340, Avg. loss: 25494080.365459\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 10748.20, NNZs: 1, Bias: 16918.123752, T: 2370, Avg. loss: 25284812.434024\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 10672.56, NNZs: 1, Bias: 16992.501139, T: 2400, Avg. loss: 25009490.948286\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 10732.49, NNZs: 1, Bias: 17083.343898, T: 2430, Avg. loss: 24960847.509969\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 10622.64, NNZs: 1, Bias: 17149.488787, T: 2460, Avg. loss: 24518315.021795\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 10571.02, NNZs: 1, Bias: 17223.806524, T: 2490, Avg. loss: 24445611.214353\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 10581.92, NNZs: 1, Bias: 17304.602134, T: 2520, Avg. loss: 24417254.423750\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 10485.23, NNZs: 1, Bias: 17367.883310, T: 2550, Avg. loss: 23698207.412818\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 10597.84, NNZs: 1, Bias: 17463.449777, T: 2580, Avg. loss: 24367752.896236\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 10533.23, NNZs: 1, Bias: 17528.028677, T: 2610, Avg. loss: 23826825.276007\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 10620.16, NNZs: 1, Bias: 17617.762398, T: 2640, Avg. loss: 23923749.842140\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 10686.22, NNZs: 1, Bias: 17704.036271, T: 2670, Avg. loss: 23595489.185575\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 10635.19, NNZs: 1, Bias: 17770.570873, T: 2700, Avg. loss: 23499668.618065\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 10852.50, NNZs: 1, Bias: 17869.159268, T: 2730, Avg. loss: 21880174.569431\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 10693.21, NNZs: 1, Bias: 17918.024125, T: 2760, Avg. loss: 23754736.068272\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 10702.09, NNZs: 1, Bias: 17991.803134, T: 2790, Avg. loss: 23118908.232310\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 10672.74, NNZs: 1, Bias: 18058.055581, T: 2820, Avg. loss: 23116979.613182\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 10632.26, NNZs: 1, Bias: 18122.165472, T: 2850, Avg. loss: 23076260.157422\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 10521.24, NNZs: 1, Bias: 18176.040240, T: 2880, Avg. loss: 22722893.305088\n",
      "Total training time: 0.13 seconds.\n",
      "Convergence after 96 epochs took 0.13 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=True,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd=SGDRegressor(verbose=True)\n",
    "sgd.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron for Regression Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def output_neuron(X,w,b):\n",
    "    yhat=X@w+b\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(yhat,ytrue):\n",
    "    return (np.sum(np.square(yhat-ytrue)))/2*len(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(X,ytrue,w,b,lr):\n",
    "    bias_derv=np.sum((X@w+b)-ytrue)/len(ytrue)\n",
    "    weight_derv=np.sum(((X@w+b)-ytrue)*X)/len(ytrue)\n",
    "    \n",
    "    new_bias=b-lr*bias_derv\n",
    "    new_weight=w-lr*weight_derv\n",
    "    return new_bias,new_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2926213275039.778\n",
      "initial parameters: [0.1399936] 0.8252375811504292\n",
      "2038053867788.4531\n",
      "638815366876.179\n",
      "635626810040.0148\n",
      "164765673890.24835\n",
      "263338004706.4884\n",
      "83799758788.14574\n",
      "155013221978.19202\n",
      "79439108933.4738\n",
      "119431177876.29643\n",
      "85499708956.65018\n",
      "106157632904.84232\n",
      "90351137343.68625\n",
      "100657826472.53621\n",
      "93149949556.98224\n",
      "98212117201.77055\n",
      "94610712103.168\n",
      "97078789635.56818\n",
      "95342823838.16325\n",
      "96541929785.6304\n",
      "95703155549.70795\n",
      "96284755012.43196\n",
      "95879010503.14108\n",
      "96160872854.68613\n",
      "95964489475.99028\n",
      "96101035582.02237\n",
      "96005958607.78189\n",
      "96072094758.32013\n",
      "96026058139.53062\n",
      "96058088254.39009\n",
      "96035795726.91\n",
      "96051307402.27356\n",
      "96040512252.33145\n",
      "96048024147.8328\n",
      "96042796521.05557\n",
      "96046434295.9906\n",
      "96043902762.77818\n",
      "96045664413.94235\n",
      "96044438488.05547\n",
      "96045291593.94234\n",
      "96044697923.34085\n",
      "96045111052.11719\n",
      "96044823559.14706\n",
      "96045023622.57443\n",
      "96044884400.18289\n",
      "96044981283.68768\n",
      "96044913863.33295\n",
      "96044960780.51501\n",
      "96044928131.27928\n",
      "96044950851.57452\n",
      "96044935040.73163\n"
     ]
    }
   ],
   "source": [
    "w=np.random.uniform(0,1,1) #here w is 1d array(lower limit,upperlimit,size)\n",
    "b=np.random.random()\n",
    "yhat=output_neuron(X,w,b)\n",
    "print(loss(yhat,y))\n",
    "print(\"initial parameters:\",w,b)\n",
    "for epoch in range(50):\n",
    "    b,w=update_parameters(X,y,w,b,.002)\n",
    "    yhat=output_neuron(X,w,b)\n",
    "    print(loss(yhat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([14287.17885289]), 90.45534334831994)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([142962.24387226,  42951.99190202])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sample=np.array([[10],[3]])\n",
    "output_neuron(new_sample,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('c:/dataset/regression/salary_2_variable.csv')\n",
    "df.Gender=df.Gender.map({'Male':0,'Female':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exp</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0</td>\n",
       "      <td>41000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>36205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>37731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "      <td>39891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0</td>\n",
       "      <td>56642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>56240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0</td>\n",
       "      <td>59150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0</td>\n",
       "      <td>61000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Exp  Gender  Salary\n",
       "0  1.1       0   41000\n",
       "1  1.3       1   36205\n",
       "2  1.5       1   37731\n",
       "3  2.0       0   43525\n",
       "4  2.2       1   39891\n",
       "5  2.9       0   56642\n",
       "6  3.0       0   60150\n",
       "7  3.2       1   56240\n",
       "8  3.3       0   59150\n",
       "9  3.8       0   61000"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(X,ytrue,w,b,lr):\n",
    "    bias_derv=np.sum((X@w+b)-ytrue)/len(ytrue)\n",
    "    weight1_derv=np.sum(((X[:,[0]]@w[0].reshape(1,-1)+b)-ytrue)*X[:,[0]])/len(ytrue)\n",
    "    weight2_derv=np.sum(((X[:,[1]]@w[1].reshape(1,-1)+b)-ytrue)*X[:,[1]])/len(ytrue)\n",
    "    \n",
    "    new_bias=b-lr*bias_derv\n",
    "    new_weight1=w[0]-lr*weight1_derv\n",
    "    new_weight2=w[1]-lr*weight2_derv\n",
    "    \n",
    "    return new_bias,np.array([new_weight1,new_weight2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial parameters:\n",
      " [0.86884571 0.29721178] 0.2726688593219957 [1.22839914 1.69938006 1.8731492  2.01036027 2.4813412  2.79232141\n",
      " 2.87920598 3.3501869  3.13985969 3.57428254]\n",
      "110244055363.86624\n",
      "31936067165.101242\n",
      "11330734107.250465\n",
      "5775761878.748712\n",
      "4314072068.553048\n",
      "4050326393.161289\n"
     ]
    }
   ],
   "source": [
    "w=np.random.uniform(0,1,2)\n",
    "b=np.random.random()\n",
    "print(\"initial parameters:\\n\",w,b,output_neuron(X,w,b))\n",
    "for epoch in range(60):\n",
    "    b,w=update_parameters(X,y,w,b,.001)\n",
    "    yhat=output_neuron(X,w,b)\n",
    "    if(epoch%10==0):\n",
    "        print(loss(yhat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([17278.74829231, 10373.10111629]), 843.9493550897515)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59591.69354894, 69964.79466523, 97610.79193292, 87237.69081663,\n",
       "       66503.19286586, 57863.81871971])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample=np.array([[3.4,0],[3.4,1],[5,1],[5,0],[3.8,0],[3.3,0]])\n",
    "output_neuron(test_sample,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
